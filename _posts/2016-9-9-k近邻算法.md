---
layout: post
title: Machine learning in Action 1——k近邻算法
date: 2016-9-9
categories: blog
tags: [Machine learning]
description: 机器学习
---

这是机器学习实战系列的第一篇，k近邻算法

K近邻算法（KNN）的工作原理：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，通常k不大于20.最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。

那么，如何计算新数据与样本数据的相似度呢？通常的做法是计算新数据与样本数据的距离（这又涉及到使用哪种距离计算，一般使用欧氏距离），距离越小，则相似度越高。

算法如下:

 1. 计算已知类别数据集中的点与当前点之间的距离；

 2. 按照距离递增次序排序；

 3.选取与当前点距离最小的k个点；

 4.确定前k个点所在的类别的出现频率；

 5.返回前k个点出现频率最高的类别作为当前点的预测分类。

**注意**：在计算距离时，由于不同特征数值量级不同，会造成一些特征对计算结果的影响较大，**为了避免由于数值大小造成的影响，需要提前对特征的取值进行单位化**。

下面为k近邻算法的python实现：

```
def classify0(inX,dataSet,labels,k):
	dataSetSize = dataSet.shape[0]
	diffMat = np.tile(inX,(dataSetSize,1))-dataSet
	sqDiffMat = diffMat**2
	sqDistances = sqDiffMat.sum(axis = 1)
	distances = sqDistances**0.5
	sortedDistIndicies = distances.argsort()
	classCount = {}
	for i in range(k):
		voteIlabel = labels[sortedDistIndicies[i]]
		classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
	sortedClassCount = sorted(classCount.iteritems(),key = operator.itemgetter(1),reverse = True)
	return sortedClassCount[0][0]

```

其中`inX`为待分类的数据点，`dataSet`为训练数据，`labels`为训练数据对应的分类标签。





